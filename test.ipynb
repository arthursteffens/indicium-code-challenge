{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "import yaml\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docker-compose.yml\", \"r\") as file:\n",
    "    yml = yaml.safe_load(file)\n",
    "\n",
    "class DBCred():\n",
    "    def __init__(self, service) -> None:\n",
    "        try:\n",
    "            db =  yml[\"services\"][service]\n",
    "            env = db[\"environment\"]\n",
    "\n",
    "            # if \"pg_db\" in service:\n",
    "            self.db_prefix = \"POSTGRES\"\n",
    "            self.eng_driver = \"postgresql+psycopg2\"\n",
    "            self.db_name = env[f\"{self.db_prefix}_DB\"]\n",
    "            self.user=env[f\"{self.db_prefix}_USER\"]\n",
    "            self.passwd=env[f\"{self.db_prefix}_PASSWORD\"]\n",
    "            self.port=yml[\"services\"][service][\"ports\"][0].split(\":\")[0]\n",
    "            self.host=\"localhost\"\n",
    "            \n",
    "            self.conn_str = f\"{self.eng_driver}://{self.user}:{self.passwd}@{self.host}:{self.port}/{self.db_name}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Invalid docker-compose.yml: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_db_cred = DBCred(\"pg_db_in\")\n",
    "dest_db_cred = DBCred(\"pg_db_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database():\n",
    "    def __init__(self, db_cred):\n",
    "        self.conn_str = db_cred.conn_str\n",
    "        self.engine = create_engine(self.conn_str)\n",
    "\n",
    "\n",
    "    def get_table_names(self, sql):\n",
    "        \"\"\"\n",
    "            Retrieve name of tables from source DB\n",
    "            return 'list' : 'table_names'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.engine.connect() as connection:\n",
    "                results = connection.execute(text(sql))\n",
    "\n",
    "            table_names = [table_name[0] for table_name in results]\n",
    "            logging.info(f\"Got {len(table_names)} tables: {table_names} \\n\")\n",
    "            return table_names\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving table names, check database status or SQL syntax: \\n{e}\")\n",
    "\n",
    "    def extract_db(self, table_names, user_date):\n",
    "        \"\"\"\n",
    "            Extract data from source DB.\n",
    "            Write each table into csv files.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.engine.connect() as connection:\n",
    "                for table in table_names:\n",
    "                    path = f\"./data/postgres/{table}/{user_date}/{table}.csv\"\n",
    "                    result = connection.execute(text(f\"SELECT * FROM {table};\"))\n",
    "                    columns = result._metadata.keys\n",
    "                    df = pd.DataFrame(columns=columns, data=result)\n",
    "                    final_df = df.astype(object).where(pd.notnull(df), 'NULL')\n",
    "                    final_df.to_csv(path, index=False, sep=',', encoding='utf-8')\n",
    "            logging.info(\"Tables extracted.\\n\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while extracting or saving data from Postgres DB: {e}\")\n",
    "\n",
    "\n",
    "    def insert_into_db(self, source, table, user_date):\n",
    "        \"\"\"\n",
    "            Insert data into destination DB.\n",
    "            return 'bool' : success\n",
    "        \"\"\"\n",
    "        success = False\n",
    "        if source == \"postgres\":\n",
    "            path = f\"./data/{source}/{table}/{user_date}/{table}.csv\"\n",
    "        elif source == \"csv\":\n",
    "            path = f\"./data/{source}/{user_date}/{table}.csv\"\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "            df.replace(to_replace=np.nan, value='NULL', inplace=True)\n",
    "\n",
    "\n",
    "            with self.engine.connect() as connection:\n",
    "                \n",
    "                df.to_sql(name=table, con=self.engine, if_exists='replace', index=False)\n",
    "                \n",
    "                connection.commit()\n",
    "            success = True\n",
    "            return success\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data into destination database: {e}\")\n",
    "\n",
    "    \n",
    "    def exec_sql(self, sql):\n",
    "        with self.engine.connect() as connection:\n",
    "            connection.execute(text(sql))\n",
    "            connection.commit()\n",
    "\n",
    "\n",
    "    def final_query(self, user_date, sql):\n",
    "        \"\"\"\n",
    "            Execute final query on destination DB and write result into 'final_query.csv'\n",
    "        \"\"\"\n",
    "        path = f\"./data/track/{user_date}/final_query.csv\"\n",
    "        with self.engine.connect() as connection:\n",
    "            result = connection.execute(text(sql))\n",
    "            cols = result._metadata.keys\n",
    "            df = pd.DataFrame(result, columns=cols)\n",
    "        df.to_csv(path, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_db = Database(source_db_cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_db = Database(dest_db_cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.constants import sql_PG_TABLE_NAMES_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tables = source_db.get_table_names(sql_PG_TABLE_NAMES_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_date = \"2023-01-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/suppliers/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/employees/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/shippers/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/categories/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/employee_territories/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/region/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/customer_demographics/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/us_states/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/products/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/territories/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/customer_customer_demo/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/customers/2023-01-30 recreated.\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/postgres/orders/2023-01-30 recreated.\n",
      "\n",
      "\n",
      "\n",
      "Step 1 already executed for this date. Reprocessing it for the selected day (2023-01-30).\n",
      "./data/csv/2023-01-30 recreated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.functions import create_db_path, create_csv_path, extract_csv\n",
    "create_db_path(src_tables, user_date)\n",
    "create_csv_path(user_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_db.extract_db(src_tables, user_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file extracted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_csv(user_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table suppliers loaded into destination database.\n",
      "Table employees loaded into destination database.\n",
      "Table shippers loaded into destination database.\n",
      "Table categories loaded into destination database.\n",
      "Table employee_territories loaded into destination database.\n",
      "Table region loaded into destination database.\n",
      "Table customer_demographics loaded into destination database.\n",
      "Table us_states loaded into destination database.\n",
      "Table products loaded into destination database.\n",
      "Table territories loaded into destination database.\n",
      "Table customer_customer_demo loaded into destination database.\n",
      "Table customers loaded into destination database.\n",
      "Table orders loaded into destination database.\n"
     ]
    }
   ],
   "source": [
    "for table in src_tables:\n",
    "    source = \"postgres\"\n",
    "    success = dest_db.insert_into_db(source, table, user_date)\n",
    "    if success:\n",
    "        print(f\"Table {table} loaded into destination database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "success = dest_db.insert_into_db(\"csv\", \"order_details\", user_date)\n",
    "if success:\n",
    "    logging.info(f\"Table {file} from CSV folder loaded into destination database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.constants import sql_PG_TABLE_NAMES_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = source_db.get_table_names(sql_PG_TABLE_NAMES_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.constants import sql_FINAL_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_db.final_query(user_date, sql_FINAL_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/constraints_db_out.sql'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"./data/constraints_db_out.sql\"\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file, \"r\") as f:\n",
    "    dest_db.exec_sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f983acfe50181cc55a8413f0e206b917d75ea4ed04a30e25c8229e8dfdf5bf18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
